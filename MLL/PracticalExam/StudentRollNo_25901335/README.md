For the base model, sklearn's Linear Regression model was used. 
We used Gradient Descent to show how an iterative optimization method works for bigger datasets where matrix inversion costs a lot of processing power.  Mean Squared Error (MSE) was the goal for training because it can be differentiated, and Mean Absolute Error (MAE) was the goal for testing because it is strong against outliers and easy to understand in original units. We used feature scaling for Gradient Descent to make sure the numbers were stable and the process went faster.  The number of epochs was changed as per the learning rate based on experience to get stable convergence without divergence. 
The closed-form normal equation was used to get the exact least-squares solution and to be a reference.
Dataset: https://www.kaggle.com/datasets/thorgodofthunder/tvradionewspaperadvertising
